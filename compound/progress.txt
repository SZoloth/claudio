## Codebase Patterns

- SwiftUI uses @Observable pattern (macOS 14+), not ObservableObject
- Settings are stored in AppSettings.swift using @AppStorage
- SettingsWriter writes to ~/.config/claudio/hook-config.sh for bash script access
- Hook script is at ~/agent-tools/brabble-claude-hook.sh
- All new Swift files must be added to project.pbxproj (PBXBuildFile, PBXFileReference, PBXGroup, PBXSourcesBuildPhase)
- MainActor dispatch required for UI updates from background threads
- Build command: ./build.sh (wraps xcodebuild)

---

## 2026-01-22 21:30 - Initial Setup

- Created PRD for Screen Context feature
- Branch: compound/screen-context
- 8 tasks to implement

---

## 2026-01-22 21:35 - SC-001 through SC-008 Complete

All 8 tasks implemented:

**Files modified:**
- `Claudio/Services/AppSettings.swift` - Added ScreenContextMode enum and screenContextMode property
- `Claudio/Views/SettingsView.swift` - Added Screen Context section with picker and mode descriptions
- `Claudio/Services/SettingsWriter.swift` - Export CLAUDIO_SCREEN_CONTEXT to hook config
- `Claudio/Views/MenuBarView.swift` - Added screen context indicator in StatusHeader
- `~/agent-tools/claudio-screenshot.sh` - New screenshot capture helper
- `~/agent-tools/brabble-claude-hook.sh` - Added trigger detection, screenshot capture, and Claude --image integration

**Learnings for future iterations:**
- SwiftUI conditional colors: Can't mix HierarchicalShapeStyle (.secondary) with Color (.blue) in ternary - use explicit Color.secondary
- Claude CLI supports --image flag for vision capabilities
- OpenAI vision API requires base64 encoding of images and specific JSON structure
- Screenshot cleanup should happen after LLM call completes to avoid race conditions

---

## 2026-01-22 21:45 - AG-001 through AG-006 Complete

All 6 Agentic Mode tasks implemented:

**Files modified:**
- `Claudio/Services/AppSettings.swift` - Added agenticMode property (Bool, default false)
- `Claudio/Views/SettingsView.swift` - Added Agentic Mode section with toggle and safety warning
- `Claudio/Services/SettingsWriter.swift` - Export CLAUDIO_AGENTIC_MODE to hook config
- `Claudio/Views/MenuBarView.swift` - Added purple "Agentic" indicator badge in StatusHeader
- `~/agent-tools/brabble-claude-hook.sh` - Conditionally remove -p flag when agentic mode enabled

**Key implementation details:**
- Agentic mode = full Claude CLI capabilities (tool use enabled)
- Prompt-only mode = -p flag (responses only, no tool access)
- Uses `echo "$prompt" | claude` pattern for agentic mode to enable tool use
- Safety warning displayed when agentic mode is enabled

**Learnings for future iterations:**
- Claude CLI: `-p` flag is prompt-only (no tools), without it enables full agentic capabilities
- Piping input via stdin (`echo | claude`) allows agentic tool use while still providing the prompt
- SourceKit errors during editing are often transient - always verify with actual build

---

## 2026-01-22 21:55 - WC-001 through WC-006 Complete

All 6 Wake Commands tasks implemented:

**Files modified/added:**
- `Claudio/Models/WakeCommand.swift` - NEW: Model with trigger, action, isEnabled
- `Claudio.xcodeproj/project.pbxproj` - Added WakeCommand.swift to build
- `Claudio/Services/AppSettings.swift` - Added wakeCommands property with JSON persistence
- `Claudio/Views/SettingsView.swift` - Added Wake Commands section with toggleable command rows
- `Claudio/Services/SettingsWriter.swift` - Export CLAUDIO_WAKE_COMMANDS as JSON
- `~/agent-tools/brabble-claude-hook.sh` - Apply wake command action prepend via jq

**Default wake commands:**
- "summarize this" → "Please provide a concise summary of the following:"
- "explain this" → "Please explain the following in simple terms:"
- "fix this" → "Please identify and fix any issues with the following:"
- "review this" → "Please provide a thorough code review of the following:"

**Learnings for future iterations:**
- JSON export with bash: Use single quotes and escape embedded single quotes with '\''
- jq for JSON parsing in bash: `jq -r --arg text "$var" '.[] | select(...)'`
- SwiftUI Binding with closure: Useful for updating array elements by reference

---

## 2026-01-22 22:10 - MCP-001 through MCP-004 Complete

All 4 MCP Server Status tasks implemented:

**Files modified/added:**
- `Claudio/Services/MCPConfigReader.swift` - NEW: Reads MCP config from ~/.claude/settings.json
- `Claudio.xcodeproj/project.pbxproj` - Added MCPConfigReader.swift to build
- `Claudio/Views/SettingsView.swift` - Added MCP Servers section showing configured servers
- `Claudio/Views/MenuBarView.swift` - Added cyan MCP badge when agentic mode + servers available

**Key implementation details:**
- MCPConfigReader parses ~/.claude/settings.json mcpServers object
- Shows server type (stdio/http) with appropriate icon
- MCP indicator only appears when agentic mode is enabled
- Info message prompts user to enable agentic mode to use MCP tools

**Learnings for future iterations:**
- Claude Code stores MCP config in ~/.claude/settings.json under "mcpServers" key
- JSONSerialization is sufficient for reading simple config files in Swift
- Computed properties in SwiftUI views for conditional display logic keeps body clean

---

## 2026-01-22 21:50 - SVR-001 through SVR-005 Complete

All 5 Streaming Voice Response tasks implemented:

**Files modified:**
- `Claudio/Services/AppSettings.swift` - Added streamingResponse property (Bool, default false)
- `Claudio/Views/SettingsView.swift` - Added streaming toggle in Behavior section (conditionally visible)
- `Claudio/Services/SettingsWriter.swift` - Export CLAUDIO_STREAMING_RESPONSE to hook config
- `~/agent-tools/brabble-claude-hook.sh` - Added speak_streaming function and conditional speech mode

**Key implementation details:**
- Streaming toggle only visible when "Speak Response" is enabled (clean UX)
- speak_streaming function splits text on sentence boundaries and speaks each sequentially
- Non-streaming mode: single `say` call with full response
- Streaming mode: sentence-by-sentence speech for faster perceived feedback
- Used rabbit/tortoise icons to visually indicate speed difference

**Learnings for future iterations:**
- macOS `say` buffers entire input before speaking - can't pipe stdin for true streaming
- Sentence-by-sentence is a good middle ground between full-buffer and word-by-word
- Conditional SwiftUI views (`if setting.enabled { ... }`) cleaner than disabled toggles

---
